# Reverse Embedding Code Synthesis
Query `Compute Wikipedia principal eigenvector for PageRank.`
## Script Variables
- n_regions_plus:<br>
>n_regions_plus is a variable that represents the number of regions that the data is split into. In
- X:<br>
>X is a numpy array of shape (1000, 1) containing random numbers between 0
- noise:<br>
>Noise is a random variable that is used to add some randomness to the data. It is added to
- sigma:<br>
>sigma is a random variable that is used to generate a lognormal distribution. It is used to add
- rng:<br>
>rng is a random number generator that is used to generate random numbers for the lognormal distribution.
- expected_y:<br>
>Expected_y is a variable that is used to generate a random noise in the form of a log
- np:<br>
>It is a library that provides a large number of mathematical functions and data structures. It is used for
- y:<br>
>The variable y is a random variable that represents the noise in the data. It is generated by adding
- PCA:<br>
>PCA is a dimensionality reduction technique that projects the data onto a lower dimensional space while retaining the most
- FastICA:<br>
>FastICA is a python implementation of the FastICA algorithm. It is a fast and efficient algorithm for
## Synthesis Blocks
### notebooks/dataset2/decomposition/plot_ica_blind_source_separation.ipynb
CONTEXT:  Fit ICA and PCA models   COMMENT:
```python
from sklearn.decomposition import PCA, FastICA
```

### notebooks/dataset2/decomposition/plot_ica_blind_source_separation.ipynb
CONTEXT:  Fit ICA and PCA models   COMMENT:
```python
from sklearn.decomposition import PCA, FastICA
```

### notebooks/dataset2/clustering/plot_coin_segmentation.ipynb
CONTEXT: Compute and visualize the resulting regions   COMMENT: Computing a few extra eigenvectors may speed up the eigen_solver. The spectral
clustering quality may also benefit from requesting extra regions for segmentation.
```python
n_regions_plus = 3
```

### notebooks/dataset2/ensemble_methods/plot_gradient_boosting_quantile.ipynb
CONTEXT: To make the problem interesting, we generate observations of the target y as the sum of a deterministic term computed by the function f and a
random noise term that follows a centered [log-normal](https://en.wikipedia.org/wiki/Log-normal_distribution). To make this even more interesting we
consider the case where the amplitude of the noise depends on the input variable x (heteroscedastic noise).  The lognormal distribution is non-
symmetric and long tailed: observing large outliers is likely but it is impossible to observe small outliers.   COMMENT:
```python
sigma = 0.5 + X.ravel() / 10
noise = rng.lognormal(sigma=sigma) - np.exp(sigma**2 / 2)
y = expected_y + noise
```

### notebooks/dataset2/ensemble_methods/plot_gradient_boosting_quantile.ipynb
CONTEXT: To make the problem interesting, we generate observations of the target y as the sum of a deterministic term computed by the function f and a
random noise term that follows a centered [log-normal](https://en.wikipedia.org/wiki/Log-normal_distribution). To make this even more interesting we
consider the case where the amplitude of the noise depends on the input variable x (heteroscedastic noise).  The lognormal distribution is non-
symmetric and long tailed: observing large outliers is likely but it is impossible to observe small outliers.   COMMENT:
```python
sigma = 0.5 + X.ravel() / 10
noise = rng.lognormal(sigma=sigma) - np.exp(sigma**2 / 2)
y = expected_y + noise
```

## Code Concatenation
```python
from sklearn.decomposition import PCA, FastICA
from sklearn.decomposition import PCA, FastICA
n_regions_plus = 3
sigma = 0.5 + X.ravel() / 10
noise = rng.lognormal(sigma=sigma) - np.exp(sigma**2 / 2)
y = expected_y + noise
sigma = 0.5 + X.ravel() / 10
noise = rng.lognormal(sigma=sigma) - np.exp(sigma**2 / 2)
y = expected_y + noise
```
