# String Code Synthesis
Query `Plot predicted probabilities vs true outcomes.`
## Script Variables
- X_test:<br>
>X_test is a numpy array of shape (n_samples, n_features) containing the test data.
- X_train:<br>
>X_train is a 2D numpy array of shape (n_samples, n_features) containing the
- y_train:<br>
>It is a numpy array containing the labels of the training data. The labels are integers from 0
- X:<br>
>X is a numpy array of shape (100000, 20) containing the features of the dataset
- y_test:<br>
>It is a test dataset that is used to evaluate the performance of the model. It is generated using
- train_test_split:<br>
>It is a function from sklearn.model_selection module which is used to split the dataset into training and testing
- y:<br>
>The variable y is a binary classification problem where the target variable is a binary classification problem where the target
- make_classification:<br>
>The make_classification function creates a dataset of random features and labels. The features are generated by sampling from
- plt:<br>
>plt is a Python library that provides a wide range of visualization tools for data analysis and visualization. It
- _:<br>
>The variable _ is used to store the legend object in the plot. It is used to label the
- x:<br>
>It is a variable that is used to iterate over a list of values. The list contains 11
## Synthesis Blocks
### notebooks/dataset2/calibration/plot_calibration_curve.ipynb
CONTEXT:   Probability Calibration curves  When performing classification one often wants to predict not only the class label, but also the associated
probability. This probability gives some kind of confidence on the prediction. This example demonstrates how to visualize how well calibrated the
predicted probabilities are using calibration curves, also known as reliability diagrams. Calibration of an uncalibrated classifier will also be
demonstrated.  COMMENT: Authors: The scikit-learn developers SPDX-License-Identifier: BSD-3-Clause
```python
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
X, y = make_classification(
    n_samples=100_000, n_features=20, n_informative=2, n_redundant=10, random_state=42
)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.99, random_state=42
)
```

### notebooks/dataset2/calibration/plot_calibration_multiclass.ipynb
CONTEXT:  Compare probabilities Below we plot a 2-simplex with arrows showing the change in predicted probabilities of the test samples.   COMMENT:
Add grid
```python
plt.grid(False)
for x in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:
    plt.plot([0, x], [x, 0], "k", alpha=0.2)
    plt.plot([0, 0 + (1 - x) / 2], [x, x + (1 - x) / 2], "k", alpha=0.2)
    plt.plot([x, x + (1 - x) / 2], [0, 0 + (1 - x) / 2], "k", alpha=0.2)
plt.title("Change of predicted probabilities on test samples after sigmoid calibration")
plt.xlabel("Probability class 1")
plt.ylabel("Probability class 2")
plt.xlim(-0.05, 1.05)
plt.ylim(-0.05, 1.05)
_ = plt.legend(loc="best")
```

## Code Concatenation
```python
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
X, y = make_classification(
    n_samples=100_000, n_features=20, n_informative=2, n_redundant=10, random_state=42
)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.99, random_state=42
)
plt.grid(False)
for x in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:
    plt.plot([0, x], [x, 0], "k", alpha=0.2)
    plt.plot([0, 0 + (1 - x) / 2], [x, x + (1 - x) / 2], "k", alpha=0.2)
    plt.plot([x, x + (1 - x) / 2], [0, 0 + (1 - x) / 2], "k", alpha=0.2)
plt.title("Change of predicted probabilities on test samples after sigmoid calibration")
plt.xlabel("Probability class 1")
plt.ylabel("Probability class 2")
plt.xlim(-0.05, 1.05)
plt.ylim(-0.05, 1.05)
_ = plt.legend(loc="best")
```
