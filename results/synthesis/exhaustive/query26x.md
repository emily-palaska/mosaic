# Exhaustive Code Synthesis
Query `Split multilabel dataset into train and test.`
## Script Variables
- plot_2d:<br>
>plot_2d is a function that takes in a matplotlib axis object and a number of labels.
- RANDOM_SEED:<br>
>RANDOM_SEED is a variable that is used to generate random data for the plot. It is a
- p_c:<br>
>The variable p_c is a 3x1 matrix which represents the probability of a point being in
- COLORS:<br>
>COLORS is an array of colors that are used to represent the different classes in the dataset. The
- p_w_c:<br>
>It is a 2x3 matrix, where each row represents the probability of a particular class (
- y_train:<br>
>y_train is a numpy array of size (n_samples, 1) containing the target values of
- X_test:<br>
>X_test is a numpy array containing the test data. It is used to plot the test data points
- X_train:<br>
>X_train is a pandas dataframe containing the features of the dataset. It is a matrix of shape (
- train_test_split:<br>
>It is a function that splits the dataset into training and testing sets. The random_state parameter is used
- X:<br>
>X is a numpy array of shape (1000, 1) containing random numbers between 0
- y_test:<br>
>It is a test set of the data used to evaluate the model's performance. It is used to
- y:<br>
>The variable y is a random variable that represents the noise in the data. It is generated by adding
## Synthesis Blocks
### notebooks/dataset2/dataset_examples/plot_random_multilabel_dataset.ipynb
CONTEXT:   Plot randomly generated multilabel dataset  This illustrates the :func:`~sklearn.datasets.make_multilabel_classification` dataset
generator. Each sample consists of counts of two features (up to 50 in total), which are differently distributed in each of two classes.  Points are
labeled as follows, where Y means the class is present:  =====  =====  =====  ======   1      2      3    Color =====  =====  =====  ======   Y      N
N    Red   N      Y      N    Blue   N      N      Y    Yellow   Y      Y      N    Purple   Y      N      Y    Orange   Y      Y      N    Green   Y
Y      Y    Brown =====  =====  =====  ======  A star marks the expected sample for each class; its size reflects the probability of selecting that
class label.  The left and right examples highlight the ``n_labels`` parameter: more of the samples in the right plot have 2 or 3 labels.  Note that
this two-dimensional example is very degenerate: generally the number of features would be much greater than the "document length", while here we have
much larger documents than vocabulary. Similarly, with ``n_classes > n_features``, it is much less likely that a feature distinguishes a particular
class.  COMMENT:
```python
def plot_2d(ax, n_labels=1, n_classes=3, length=50):    X, Y, p_c, p_w_c = make_ml_clf(        n_samples=150,        n_features=2,        n_classes=n_classes,        n_labels=n_labels,        length=length,        allow_unlabeled=False,        return_distributions=True,        random_state=RANDOM_SEED,    )    ax.scatter(        X[:, 0], X[:, 1], color=COLORS.take((Y * [1, 2, 4]).sum(axis=1)), marker="."    )    ax.scatter(        p_w_c[0] * length,        p_w_c[1] * length,        marker="*",        linewidth=0.5,        edgecolor="black",        s=20 + 1500 * p_c**2,        color=COLORS.take([1, 2, 4]),    )    ax.set_xlabel("Feature 0 count")    return p_c, p_w_c
```

### notebooks/dataset2/ensemble_methods/plot_gradient_boosting_quantile.ipynb
CONTEXT: Split into train, test datasets:   COMMENT:
```python
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
```

## Code Concatenation
```python
def plot_2d(ax, n_labels=1, n_classes=3, length=50):    X, Y, p_c, p_w_c = make_ml_clf(        n_samples=150,        n_features=2,        n_classes=n_classes,        n_labels=n_labels,        length=length,        allow_unlabeled=False,        return_distributions=True,        random_state=RANDOM_SEED,    )    ax.scatter(        X[:, 0], X[:, 1], color=COLORS.take((Y * [1, 2, 4]).sum(axis=1)), marker="."    )    ax.scatter(        p_w_c[0] * length,        p_w_c[1] * length,        marker="*",        linewidth=0.5,        edgecolor="black",        s=20 + 1500 * p_c**2,        color=COLORS.take([1, 2, 4]),    )    ax.set_xlabel("Feature 0 count")    return p_c, p_w_c
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
```
