# Embedding Code Synthesis
Query `Compute Wikipedia principal eigenvector for PageRank.`
## Script Variables
- n_regions_plus:<br>
>n_regions_plus is a variable that represents the number of regions that the data is split into. In
- n_clusters:<br>
>n_clusters is a variable that is used to determine the number of clusters that will be created. It
- plt:<br>
>plt is a module that provides a number of functions for creating and manipulating plots.
- float:<br>
>The variable float is a floating-point number that is used to represent decimal numbers in Python. It is
- rescaled_coins:<br>
>The rescaled_coins variable is a 2D numpy array that represents the input image. It
- label:<br>
>The variable label is a 2D numpy array of size (100, 100) which represents
- l:<br>
>l is a variable that is used to iterate over the range of n_clusters.
- range:<br>
>The variable range is from 0 to 1. The range of values for each variable is determined
- i:<br>
>The variable i is used to index the important words in the bicluster. It is used to
- cocluster:<br>
>Cocluster is a variable that is used to identify the documents that are not part of the cluster
- bicluster_ncut:<br>
>It is a function that calculates the number of clusters in the bicluster.
- np:<br>
>np is a python module that contains many functions and classes for working with arrays. It is used to
- X:<br>
>X is a numpy array of shape (n_samples, n_features) where n_samples is the number
- len:<br>
>len is a function that returns the length of an object. In this case, it is used to
- categorical_columns_subset:<br>
>It is a list of strings that contains the names of the columns in the dataset that are categorical in
- print:<br>
>The variable print is used to print the number of samples, number of features, number of categorical features
- numerical_columns_subset:<br>
>numerical_columns_subset is a list of numerical columns that are used to train the model. It is
- n_categorical_features:<br>
>It is the number of categorical features in the dataset. It is used to determine the number of categories
- n_numerical_features:<br>
>n_numerical_features is the number of numerical features in the dataset. It is used to determine the
- n_features:<br>
>It is the number of features used in the PCA model. In this case, it is the number
- rank:<br>
>Rank is the number of components that are selected by the cross validation method. It is the number of
- rng:<br>
>rng is a random number generator. It is used to generate random numbers for the purpose of creating a
- linalg:<br>
>The variable linalg is a module that provides a variety of linear algebra functions. It is a part
- n_samples:<br>
>n_samples is the number of samples in the dataset. It is used to generate random noise for the
- U:<br>
>U is a matrix of size n_features x rank. It is a matrix of random numbers generated by
- _:<br>
>The variable _ is used to store the result of the svd function in scipy. It is a
- sigma:<br>
>sigma is a random number that is used to generate random numbers from a normal distribution. It is used
## Synthesis Blocks
### notebooks/dataset2/clustering/plot_coin_ward_segmentation.ipynb
CONTEXT:  Plot the results on an image  Agglomerative clustering is able to segment each coin however, we have had to use a ``n_cluster`` larger than
the number of coins because the segmentation is finding a large in the background.   COMMENT:
```python
import matplotlib.pyplot as plt
plt.figure(figsize=(5, 5))
plt.imshow(rescaled_coins, cmap=plt.cm.gray)
for l in range(n_clusters):
    plt.contour(
        label == l,
        colors=[
            plt.cm.nipy_spectral(l / float(n_clusters)),
        ],
    )
plt.axis("off")
plt.show()
```

### notebooks/dataset2/biclustering/plot_bicluster_newsgroups.ipynb
CONTEXT:   Biclustering documents with the Spectral Co-clustering algorithm  This example demonstrates the Spectral Co-clustering algorithm on the
twenty newsgroups dataset. The 'comp.os.ms-windows.misc' category is excluded because it contains many posts containing nothing but data.  The TF-IDF
vectorized posts form a word frequency matrix, which is then biclustered using Dhillon's Spectral Co-Clustering algorithm. The resulting document-word
biclusters indicate subsets words used more often in those subsets documents.  For a few of the best biclusters, its most common document categories
and its ten most important words get printed. The best biclusters are determined by their normalized cut. The best words are determined by comparing
their sums inside and outside the bicluster.  For comparison, the documents are also clustered using MiniBatchKMeans. The document clusters derived
from the biclusters achieve a better V-measure than clusters found by MiniBatchKMeans.  COMMENT: Note: the following is identical to X[rows[:,
np.newaxis], cols].sum() but much faster in scipy <= 0.16
```python
def bicluster_ncut(i):    rows, cols = cocluster.get_indices(i)    if not (np.any(rows) and np.any(cols)):        import sys        return sys.float_info.max    row_complement = np.nonzero(np.logical_not(cocluster.rows_[i]))[0]    col_complement = np.nonzero(np.logical_not(cocluster.columns_[i]))[0]    weight = X[rows][:, cols].sum()    cut = X[row_complement][:, cols].sum() + X[rows][:, col_complement].sum()    return cut / weight
```

### notebooks/dataset2/decomposition/plot_pca_vs_fa_model_selection.ipynb
CONTEXT:  Create the data   COMMENT:
```python
import numpy as np
from scipy import linalg
n_samples, n_features, rank = 500, 25, 5
sigma = 1.0
rng = np.random.RandomState(42)
U, _, _ = linalg.svd(rng.randn(n_features, n_features))
X = np.dot(rng.randn(n_samples, rank), U[:, :rank].T)
```

### notebooks/dataset2/clustering/plot_coin_segmentation.ipynb
CONTEXT: Compute and visualize the resulting regions   COMMENT: Computing a few extra eigenvectors may speed up the eigen_solver. The spectral
clustering quality may also benefit from requesting extra regions for segmentation.
```python
n_regions_plus = 3
```

### notebooks/dataset2/ensemble_methods/plot_gradient_boosting_categorical.ipynb
CONTEXT:  Load Ames Housing dataset First, we load the Ames Housing data as a pandas dataframe. The features are either categorical or numerical:
COMMENT: Select only a subset of features of X to make the example faster to run
```python
categorical_columns_subset = [
    "BldgType",
    "GarageFinish",
    "LotConfig",
    "Functional",
    "MasVnrType",
    "HouseStyle",
    "FireplaceQu",
    "ExterCond",
    "ExterQual",
    "PoolQC",
]
numerical_columns_subset = [
    "3SsnPorch",
    "Fireplaces",
    "BsmtHalfBath",
    "HalfBath",
    "GarageCars",
    "TotRmsAbvGrd",
    "BsmtFinSF1",
    "BsmtFinSF2",
    "GrLivArea",
    "ScreenPorch",
]
X = X[categorical_columns_subset + numerical_columns_subset]
X[categorical_columns_subset] = X[categorical_columns_subset].astype("category")
categorical_columns_subset = X.select_dtypes(include="category").columns
n_categorical_features = len(categorical_columns_subset)
n_numerical_features = X.select_dtypes(include="number").shape[1]
print(f"Number of samples: {X.shape[0]}")
print(f"Number of features: {X.shape[1]}")
print(f"Number of categorical features: {n_categorical_features}")
print(f"Number of numerical features: {n_numerical_features}")
```

## Code Concatenation
```python
import matplotlib.pyplot as plt
plt.figure(figsize=(5, 5))
plt.imshow(rescaled_coins, cmap=plt.cm.gray)
for l in range(n_clusters):
    plt.contour(
        label == l,
        colors=[
            plt.cm.nipy_spectral(l / float(n_clusters)),
        ],
    )
plt.axis("off")
plt.show()
def bicluster_ncut(i):    rows, cols = cocluster.get_indices(i)    if not (np.any(rows) and np.any(cols)):        import sys        return sys.float_info.max    row_complement = np.nonzero(np.logical_not(cocluster.rows_[i]))[0]    col_complement = np.nonzero(np.logical_not(cocluster.columns_[i]))[0]    weight = X[rows][:, cols].sum()    cut = X[row_complement][:, cols].sum() + X[rows][:, col_complement].sum()    return cut / weight
import numpy as np
from scipy import linalg
n_samples, n_features, rank = 500, 25, 5
sigma = 1.0
rng = np.random.RandomState(42)
U, _, _ = linalg.svd(rng.randn(n_features, n_features))
X = np.dot(rng.randn(n_samples, rank), U[:, :rank].T)
n_regions_plus = 3
categorical_columns_subset = [
    "BldgType",
    "GarageFinish",
    "LotConfig",
    "Functional",
    "MasVnrType",
    "HouseStyle",
    "FireplaceQu",
    "ExterCond",
    "ExterQual",
    "PoolQC",
]
numerical_columns_subset = [
    "3SsnPorch",
    "Fireplaces",
    "BsmtHalfBath",
    "HalfBath",
    "GarageCars",
    "TotRmsAbvGrd",
    "BsmtFinSF1",
    "BsmtFinSF2",
    "GrLivArea",
    "ScreenPorch",
]
X = X[categorical_columns_subset + numerical_columns_subset]
X[categorical_columns_subset] = X[categorical_columns_subset].astype("category")
categorical_columns_subset = X.select_dtypes(include="category").columns
n_categorical_features = len(categorical_columns_subset)
n_numerical_features = X.select_dtypes(include="number").shape[1]
print(f"Number of samples: {X.shape[0]}")
print(f"Number of features: {X.shape[1]}")
print(f"Number of categorical features: {n_categorical_features}")
print(f"Number of numerical features: {n_numerical_features}")
```
