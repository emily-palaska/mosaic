# Embedding Code Synthesis
Query `Compare cluster shapes using different methods.`
## Script Variables
- KMeans:<br>
>KMeans is a clustering algorithm that uses an iterative approach to partition n observations into k clusters, where
- BisectingKMeans:<br>
>BisectingKMeans is a clustering algorithm that uses a divide and conquer approach to find the
- clustering_algorithms:<br>
>It is a dictionary that contains the name of the clustering algorithm and the corresponding class that implements it.
- random_state:<br>
>The random_state variable is a seed value used to initialize the random number generator. It is used to
- X:<br>
>X is a 2D array of shape (n_samples, n_features) containing the input data
- n_samples:<br>
>n_samples is the number of samples used in the experiment. It is a constant value that is used
- make_blobs:<br>
>make_blobs is a function that generates a dataset of points in a 2D space. The
- _:<br>
>It is a variable that is used to store the number of samples in the dataset. It is used
- centers:<br>
>The variable centers is a list of tuples, where each tuple represents the coordinates of a cluster center.
- names:<br>
>n_samples
- score_name:<br>
>The score_name is a variable that is used to store the name of the score function that is being
- plots:<br>
>Variable plots is a variable that stores the plots generated by the script. It is a list of matplotlib
- plt:<br>
>plt is a module in python which is used to plot graphs. It is used to create graphs,
- scores:<br>
>It is a 2D numpy array of size (len(n_clusters_range), n_runs) where
- n_clusters_range:<br>
>It is a list of integers that represents the number of clusters to be generated for the random labels.
- score_funcs:<br>
>It is a list of tuples containing the name of the scoring function and a reference to the function itself
- marker:<br>
>The variable marker is used to represent the different types of markers that can be used to represent the different
- np:<br>
>The np is an abbreviation for the Python package NumPy. NumPy is a library for the Python
- zip:<br>
>The zip function is used to create a list of tuples where the first element of each tuple is the
- int:<br>
>int is a data type that is used to store integer values. It is a 32-bit signed
- score_func:<br>
>The score_func variable is a dictionary that contains different clustering measures. The values of the dictionary are functions
- uniform_labelings_scores:<br>
>It is a function that takes in a function score_func, number of samples n_samples, and number
## Synthesis Blocks
### notebooks/dataset2/clustering/plot_bisect_kmeans.ipynb
CONTEXT:   Bisecting K-Means and Regular K-Means Performance Comparison  This example shows differences between Regular K-Means algorithm and
Bisecting K-Means.  While K-Means clusterings are different when increasing n_clusters, Bisecting K-Means clustering builds on top of the previous
ones. As a result, it tends to create clusters that have a more regular large-scale structure. This difference can be visually observed: for all
numbers of clusters, there is a dividing line cutting the overall data cloud in two for BisectingKMeans, which is not present for regular K-Means.
COMMENT: Generate sample data
```python
n_samples = 10000
random_state = 0
X, _ = make_blobs(n_samples=n_samples, centers=2, random_state=random_state)
```

### notebooks/dataset2/clustering/plot_bisect_kmeans.ipynb
CONTEXT:   Bisecting K-Means and Regular K-Means Performance Comparison  This example shows differences between Regular K-Means algorithm and
Bisecting K-Means.  While K-Means clusterings are different when increasing n_clusters, Bisecting K-Means clustering builds on top of the previous
ones. As a result, it tends to create clusters that have a more regular large-scale structure. This difference can be visually observed: for all
numbers of clusters, there is a dividing line cutting the overall data cloud in two for BisectingKMeans, which is not present for regular K-Means.
COMMENT: Algorithms to compare
```python
clustering_algorithms = {
    "Bisecting K-Means": BisectingKMeans,
    "K-Means": KMeans,
}
```

### notebooks/dataset2/clustering/plot_adjusted_for_chance_measures.ipynb
CONTEXT: In this case, we use `n_samples=100` to show the effect of having a number of clusters similar or equal to the number of samples.   COMMENT:
```python
n_samples = 100
n_clusters_range = np.linspace(2, n_samples, 10).astype(int)
plt.figure(2)
plots = []
names = []
for marker, (score_name, score_func) in zip("d^vx.,", score_funcs):
    scores = uniform_labelings_scores(score_func, n_samples, n_clusters_range)
    plots.append(
        plt.errorbar(
            n_clusters_range,
            np.median(scores, axis=1),
            scores.std(axis=1),
            alpha=0.8,
            linewidth=2,
            marker=marker,
        )[0]
    )
    names.append(score_name)
plt.title(
    "Clustering measures for 2 random uniform labelings\nwith equal number of clusters"
)
plt.xlabel(f"Number of clusters (Number of samples is fixed to {n_samples})")
plt.ylabel("Score value")
plt.legend(plots, names)
plt.ylim(bottom=-0.05, top=1.05)
plt.show()
```

## Code Concatenation
```python
n_samples = 10000
random_state = 0
X, _ = make_blobs(n_samples=n_samples, centers=2, random_state=random_state)
clustering_algorithms = {
    "Bisecting K-Means": BisectingKMeans,
    "K-Means": KMeans,
}
n_samples = 100
n_clusters_range = np.linspace(2, n_samples, 10).astype(int)
plt.figure(2)
plots = []
names = []
for marker, (score_name, score_func) in zip("d^vx.,", score_funcs):
    scores = uniform_labelings_scores(score_func, n_samples, n_clusters_range)
    plots.append(
        plt.errorbar(
            n_clusters_range,
            np.median(scores, axis=1),
            scores.std(axis=1),
            alpha=0.8,
            linewidth=2,
            marker=marker,
        )[0]
    )
    names.append(score_name)
plt.title(
    "Clustering measures for 2 random uniform labelings\nwith equal number of clusters"
)
plt.xlabel(f"Number of clusters (Number of samples is fixed to {n_samples})")
plt.ylabel("Score value")
plt.legend(plots, names)
plt.ylim(bottom=-0.05, top=1.05)
plt.show()
```
