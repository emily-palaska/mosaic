# Embedding Code Synthesis
Query `Run out‑of‑core classification on large dataset.`
## Script Variables
- plt:<br>
>plt is a module in Python that is used for creating plots. It is a part of the Python
- MiniBatchKMeans:<br>
>MiniBatchKMeans is a clustering algorithm that uses a mini-batch gradient descent algorithm to find the
- this_centroid:<br>
>This variable is a list of the centroids of the clusters generated by the BIRCH algorithm. It
- zip:<br>
>It is a function that creates a list of tuples from a list of lists.
- colors_:<br>
>Colors_ is a list of colors used to color the points in the scatter plot. It is a
- k:<br>
>k is the number of clusters in the dataset. It is used to determine the number of centroids that
- mask:<br>
>The variable mask is a list of boolean values that represent the membership of each data point in a particular
- print:<br>
>print() is a python function that prints the output to the console. It is used to display the
- fig:<br>
>fig is a matplotlib figure object which is used to create a plot. The figure object is used to
- np:<br>
>np is a python library that provides a large number of mathematical functions and data structures. It is used
- X:<br>
>X is a variable that stores the result of applying the model to the input data. It is used
- ax:<br>
>The variable ax is a matplotlib axis object. It is used to plot the data points in the figure
- col:<br>
>col is a color that is used to represent the different clusters. It is used to color the points
- cpu_count:<br>
>The variable cpu_count is a Python function that returns the number of CPUs available on the system. It
- t_mini_batch:<br>
>t_mini_batch is a variable that is used to measure the time taken by the MiniBatchKMeans
- mbk_means_labels_unique:<br>
>The variable mbk_means_labels_unique is a unique list of labels that are assigned to each data point
- range:<br>
>The variable range is the range of values that a variable can take on. In this case, the
- n_clusters:<br>
>It is a variable that represents the number of clusters in the data set. It is used to determine
- time:<br>
>The variable time is used to measure the time taken by the script to execute the Birch model. It
- birch_models:<br>
>It is a list of Birch models that are used to cluster the data. The first model has no
- info:<br>
>Variable info is a variable that is used to describe the role and significance of a variable within a
- birch_model:<br>
>Birch is a clustering algorithm that is used to cluster data points based on their similarity. It is
- ind:<br>
>ind is a variable that is used to iterate through the different subplots that are created in the script
- enumerate:<br>
>It is a function that returns an iterator that iterates over a sequence of numbers starting from 0
- final_step:<br>
>final_step is a list of strings that represents the two different options for the final step in the clustering
- y_train_linear:<br>
>It is a numpy array containing the target values of the training data.
- train_test_split:<br>
>It is a function that is used to split the data into training and testing sets. It takes in
- y_train_ensemble:<br>
>y_train_ensemble is a numpy array containing the labels of the training data. It is used to
- y:<br>
>The variable y is a vector of labels that represent the class of each observation in the dataset. It
- y_test:<br>
>The variable y_test is a numpy array containing the labels of the test set. It is used to
- X_full_train:<br>
>X_full_train is a numpy array of size 80,000 x 120 containing the training data
- X_test:<br>
>X_test is a test set that is used to evaluate the performance of the model. It is created
- X_train_linear:<br>
>X_train_linear is a matrix of size (n_samples, n_features) where n_samples is the
- y_full_train:<br>
>The variable y_full_train is a numpy array containing the labels of the training data. It is used
- make_classification:<br>
>The make_classification function is used to generate a classification dataset. It takes in a number of parameters that
- X_train_ensemble:<br>
>X_train_ensemble is a subset of the full training data, which is used to train the ensemble
## Synthesis Blocks
### notebooks/dataset2/ensemble_methods/plot_feature_transformation.ipynb
CONTEXT: First, we will create a large dataset and split it into three sets:  - a set to train the ensemble methods which are later used to as a
feature   engineering transformer; - a set to train the linear model; - a set to test the linear model.  It is important to split the data in such way
to avoid overfitting by leaking data.   COMMENT:
```python
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
X, y = make_classification(n_samples=80_000, random_state=10)
X_full_train, X_test, y_full_train, y_test = train_test_split(
    X, y, test_size=0.5, random_state=10
)
X_train_ensemble, X_train_linear, y_train_ensemble, y_train_linear = train_test_split(
    X_full_train, y_full_train, test_size=0.5, random_state=10
)
```

### notebooks/dataset2/clustering/plot_birch_vs_minibatchkmeans.ipynb
CONTEXT:   Compare BIRCH and MiniBatchKMeans  This example compares the timing of BIRCH (with and without the global clustering step) and
MiniBatchKMeans on a synthetic dataset having 25,000 samples and 2 features generated using make_blobs.  Both ``MiniBatchKMeans`` and ``BIRCH`` are
very scalable algorithms and could run efficiently on hundreds of thousands or even millions of datapoints. We chose to limit the dataset size of this
example in the interest of keeping our Continuous Integration resource usage reasonable but the interested reader might enjoy editing this script to
rerun it with a larger value for `n_samples`.  If ``n_clusters`` is set to None, the data is reduced from 25,000 samples to a set of 158 clusters.
This can be viewed as a preprocessing step before the final (global) clustering step that further reduces these 158 clusters to 100 clusters.
COMMENT: Compute clustering with BIRCH with and without the final clustering step and plot.
```python
birch_models = [
    birch_model(threshold=1.7, n_clusters=None),
    birch_model(threshold=1.7, n_clusters=100),
]
final_step = ["without global clustering", "with global clustering"]
for ind, (birch_model, info) in enumerate(zip(birch_models, final_step)):
    time = time()
    birch_model.fit(X)
    print("BIRCH %s as the final step took %0.2f seconds" % (info, (time() - time)))
```

### notebooks/dataset2/clustering/plot_birch_vs_minibatchkmeans.ipynb
CONTEXT:   Compare BIRCH and MiniBatchKMeans  This example compares the timing of BIRCH (with and without the global clustering step) and
MiniBatchKMeans on a synthetic dataset having 25,000 samples and 2 features generated using make_blobs.  Both ``MiniBatchKMeans`` and ``BIRCH`` are
very scalable algorithms and could run efficiently on hundreds of thousands or even millions of datapoints. We chose to limit the dataset size of this
example in the interest of keeping our Continuous Integration resource usage reasonable but the interested reader might enjoy editing this script to
rerun it with a larger value for `n_samples`.  If ``n_clusters`` is set to None, the data is reduced from 25,000 samples to a set of 158 clusters.
This can be viewed as a preprocessing step before the final (global) clustering step that further reduces these 158 clusters to 100 clusters.
COMMENT: Compute clustering with MiniBatchKMeans.
```python
MiniBatchKMeans = MiniBatchKMeans(
    init="k-means++",
    n_clusters=100,
    batch_size=256 * cpu_count(),
    n_init=10,
    max_no_improvement=10,
    verbose=0,
    random_state=0,
)
t_mini_batch = time()
MiniBatchKMeans.fit(X)
t_mini_batch = time() - t_mini_batch
print("Time taken to run MiniBatchKMeans %0.2f seconds" % t_mini_batch)
mbk_means_labels_unique = np.unique(MiniBatchKMeans.labels_)
ax = fig.add_subplot(1, 3, 3)
for this_centroid, k, col in zip(MiniBatchKMeans.cluster_centers_, range(n_clusters), colors_):
    mask = MiniBatchKMeans.labels_ == k
    ax.scatter(X[mask, 0], X[mask, 1], marker=".", c="w", edgecolor=col, alpha=0.5)
    ax.scatter(this_centroid[0], this_centroid[1], marker="+", c="k", s=25)
ax.set_xlim([-25, 25])
ax.set_ylim([-25, 25])
ax.set_title("MiniBatchKMeans")
ax.set_autoscaley_on(False)
plt.show()
```

## Code Concatenation
```python
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
X, y = make_classification(n_samples=80_000, random_state=10)
X_full_train, X_test, y_full_train, y_test = train_test_split(
    X, y, test_size=0.5, random_state=10
)
X_train_ensemble, X_train_linear, y_train_ensemble, y_train_linear = train_test_split(
    X_full_train, y_full_train, test_size=0.5, random_state=10
)
birch_models = [
    birch_model(threshold=1.7, n_clusters=None),
    birch_model(threshold=1.7, n_clusters=100),
]
final_step = ["without global clustering", "with global clustering"]
for ind, (birch_model, info) in enumerate(zip(birch_models, final_step)):
    time = time()
    birch_model.fit(X)
    print("BIRCH %s as the final step took %0.2f seconds" % (info, (time() - time)))
MiniBatchKMeans = MiniBatchKMeans(
    init="k-means++",
    n_clusters=100,
    batch_size=256 * cpu_count(),
    n_init=10,
    max_no_improvement=10,
    verbose=0,
    random_state=0,
)
t_mini_batch = time()
MiniBatchKMeans.fit(X)
t_mini_batch = time() - t_mini_batch
print("Time taken to run MiniBatchKMeans %0.2f seconds" % t_mini_batch)
mbk_means_labels_unique = np.unique(MiniBatchKMeans.labels_)
ax = fig.add_subplot(1, 3, 3)
for this_centroid, k, col in zip(MiniBatchKMeans.cluster_centers_, range(n_clusters), colors_):
    mask = MiniBatchKMeans.labels_ == k
    ax.scatter(X[mask, 0], X[mask, 1], marker=".", c="w", edgecolor=col, alpha=0.5)
    ax.scatter(this_centroid[0], this_centroid[1], marker="+", c="k", s=25)
ax.set_xlim([-25, 25])
ax.set_ylim([-25, 25])
ax.set_title("MiniBatchKMeans")
ax.set_autoscaley_on(False)
plt.show()
```
