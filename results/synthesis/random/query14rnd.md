# Random Code Synthesis
Query `Plot calibration curves for multiple classifiers.`
## Script Variables
- X_test:<br>
>X_test is a matrix containing the test data. It is used to test the model trained on the
- y_test:<br>
>The variable y_test is a test set that is used to evaluate the performance of the model. It
- pcr:<br>
>pcr is a Pipeline object that is used to perform a PCA analysis on the input data. It
- print:<br>
>The print function is used to display the output of the script to the console. It takes a single
- pls:<br>
>pls is a variable that is used to store the PLS regression model. It is used to predict
- label:<br>
>The variable label is a unique identifier for each observation in the dataset. It is used to identify the
- np:<br>
>It is a library that provides a large number of mathematical functions and data structures. It is used for
- setting:<br>
>The variable setting is used to specify the parameters of the gradient boosting classifier. It includes the number of
- X:<br>
>X is a 2D array of size 1000x2. It represents the data points
- labels:<br>
>labels
- train_test_split:<br>
>The train_test_split function is used to split the data into training and testing sets. It takes in
- y_train:<br>
>y_train is a numpy array containing the labels of the training data. It is used to train the
- ensemble:<br>
>The variable ensemble is a collection of variables that are used to represent a dataset. It is a set
- X_train:<br>
>X_train is a numpy array of size (n_samples, n_features) where n_samples is the
- plt:<br>
>plt is a python library that is used to create plots and graphs. It is a powerful and flexible
- clf:<br>
>clf is a gradient boosting classifier that is used to predict the class of a given sample. It is
- dict:<br>
>dict is a python dictionary which is a collection of key-value pairs.
- original_params:<br>
>It is a dictionary containing the parameters for the gradient boosting classifier. The parameters are
- y:<br>
>The variable y is a random variable that represents the noise in the data. It is generated by adding
- color:<br>
>The variable color is used to represent the different colors of the different trees in the forest. The color
- coverage_fraction:<br>
>The coverage_fraction variable is used to calculate the coverage of the prediction of the model. It is calculated
- y_high:<br>
>The variable y_high is a user defined variable which is used to calculate the coverage fraction of the data
- y_low:<br>
>The variable y_low is the lower bound of the range of values that the variable y must fall within
- electricity:<br>
>The variable electricity is a dataset containing information about the consumption of electricity in a residential area. The dataset
- df:<br>
>The variable df is a pandas dataframe that contains the electricity dataset.
- fetch_openml:<br>
>The fetch_openml function is used to fetch data from the OpenML dataset repository. It takes in
- plot:<br>
>Plot is a variable that is used to plot the data points in the given dataset. It is used
- hdb:<br>
>It is a HDBSCAN() object. HDBSCAN() is a clustering algorithm that uses a
- HDBSCAN:<br>
>HDBSCAN is a clustering algorithm that uses a hierarchical density-based approach to cluster data. It is
## Synthesis Blocks
### notebooks/dataset2/ensemble_methods/plot_hgbt_regression.ipynb
CONTEXT:  Preparing the data The [electricity dataset](http://www.openml.org/d/151) consists of data collected from the Australian New South Wales
Electricity Market. In this market, prices are not fixed and are affected by supply and demand. They are set every five minutes. Electricity transfers
to/from the neighboring state of Victoria were done to alleviate fluctuations.  The dataset, originally named ELEC2, contains 45,312 instances dated
from 7 May 1996 to 5 December 1998. Each sample of the dataset refers to a period of 30 minutes, i.e. there are 48 instances for each time period of
one day. Each sample on the dataset has 7 columns:  - date: between 7 May 1996 to 5 December 1998. Normalized between 0 and 1; - day: day of week
(1-7); - period: half hour intervals over 24 hours. Normalized between 0 and 1; - nswprice/nswdemand: electricity price/demand of New South Wales; -
vicprice/vicdemand: electricity price/demand of Victoria.  Originally, it is a classification task, but here we use it for the regression task to
predict the scheduled electricity transfer between states.   COMMENT:
```python
from sklearn.datasets import fetch_openml
electricity = fetch_openml(
    name="electricity", version=1, as_frame=True, parser="pandas"
)
df = electricity.frame
```

### notebooks/dataset2/clustering/plot_hdbscan.ipynb
CONTEXT: To properly cluster the two dense clusters, we would need a smaller value of epsilon, however at `eps=0.3` we are already fragmenting the
sparse clusters, which would only become more severe as we decrease epsilon. Indeed it seems that DBSCAN is incapable of simultaneously separating the
two dense clusters while preventing the sparse clusters from fragmenting. Let's compare with HDBSCAN.   COMMENT:
```python
hdb = HDBSCAN().fit(X)
plot(X, hdb.labels_, hdb.probabilities_)
```

### notebooks/dataset2/ensemble_methods/plot_gradient_boosting_quantile.ipynb
CONTEXT: Errors are higher meaning the models slightly overfitted the data. It still shows that the best test metric is obtained when the model is
trained by minimizing this same metric.  Note that the conditional median estimator is competitive with the squared error estimator in terms of MSE on
the test set: this can be explained by the fact the squared error estimator is very sensitive to large outliers which can cause significant
overfitting. This can be seen on the right hand side of the previous plot. The conditional median estimator is biased (underestimation for this
asymmetric noise) but is also naturally robust to outliers and overfits less.    Calibration of the confidence interval  We can also evaluate the
ability of the two extreme quantile estimators at producing a well-calibrated conditional 90%-confidence interval.  To do this we can compute the
fraction of observations that fall between the predictions:   COMMENT:
```python
def coverage_fraction(y, y_low, y_high):    return np.mean(np.logical_and(y >= y_low, y <= y_high))
```

### notebooks/dataset2/cross_decomposition/plot_pcr_vs_pls.ipynb
CONTEXT:  Projection on one component and predictive power  We now create two regressors: PCR and PLS, and for our illustration purposes we set the
number of components to 1. Before feeding the data to the PCA step of PCR, we first standardize it, as recommended by good practice. The PLS estimator
has built-in scaling capabilities.  For both models, we plot the projected data onto the first component against the target. In both cases, this
projected data is what the regressors will use as training data.   COMMENT:
```python
print(f"PCR r-squared {pcr.score(X_test, y_test):.3f}")
print(f"PLS r-squared {pls.score(X_test, y_test):.3f}")
```

### notebooks/dataset2/ensemble_methods/plot_gradient_boosting_regularization.ipynb
CONTEXT:   Gradient Boosting regularization  Illustration of the effect of different regularization strategies for Gradient Boosting. The example is
taken from Hastie et al 2009 [1]_.  The loss function used is binomial deviance. Regularization via shrinkage (``learning_rate < 1.0``) improves
performance considerably. In combination with shrinkage, stochastic gradient boosting (``subsample < 1.0``) can produce more accurate models by
reducing the variance via bagging. Subsampling without shrinkage usually does poorly. Another strategy to reduce the variance is by subsampling the
features analogous to the random splits in Random Forests (via the ``max_features`` parameter).  .. [1] T. Hastie, R. Tibshirani and J. Friedman,
"Elements of Statistical     Learning Ed. 2", Springer, 2009.  COMMENT: map labels from {-1, 1} to {0, 1}
```python
labels, y = np.unique(y, return_inverse=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=0)
original_params = {
    "n_estimators": 400,
    "max_leaf_nodes": 4,
    "max_depth": None,
    "random_state": 2,
    "min_samples_split": 5,
}
plt.figure()
for label, color, setting in [
    ("No shrinkage", "orange", {"learning_rate": 1.0, "subsample": 1.0}),
    ("learning_rate=0.2", "turquoise", {"learning_rate": 0.2, "subsample": 1.0}),
    ("subsample=0.5", "blue", {"learning_rate": 1.0, "subsample": 0.5}),
    (
        "learning_rate=0.2, subsample=0.5",
        "gray",
        {"learning_rate": 0.2, "subsample": 0.5},
    ),
    (
        "learning_rate=0.2, max_features=2",
        "magenta",
        {"learning_rate": 0.2, "max_features": 2},
    ),
]:
    setting = dict(original_params)
    setting.update(setting)
    clf = ensemble.GradientBoostingClassifier(**setting)
    clf.fit(X_train, y_train)
```

## Code Concatenation
```python
from sklearn.datasets import fetch_openml
electricity = fetch_openml(
    name="electricity", version=1, as_frame=True, parser="pandas"
)
df = electricity.frame
hdb = HDBSCAN().fit(X)
plot(X, hdb.labels_, hdb.probabilities_)
def coverage_fraction(y, y_low, y_high):    return np.mean(np.logical_and(y >= y_low, y <= y_high))
print(f"PCR r-squared {pcr.score(X_test, y_test):.3f}")
print(f"PLS r-squared {pls.score(X_test, y_test):.3f}")
labels, y = np.unique(y, return_inverse=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=0)
original_params = {
    "n_estimators": 400,
    "max_leaf_nodes": 4,
    "max_depth": None,
    "random_state": 2,
    "min_samples_split": 5,
}
plt.figure()
for label, color, setting in [
    ("No shrinkage", "orange", {"learning_rate": 1.0, "subsample": 1.0}),
    ("learning_rate=0.2", "turquoise", {"learning_rate": 0.2, "subsample": 1.0}),
    ("subsample=0.5", "blue", {"learning_rate": 1.0, "subsample": 0.5}),
    (
        "learning_rate=0.2, subsample=0.5",
        "gray",
        {"learning_rate": 0.2, "subsample": 0.5},
    ),
    (
        "learning_rate=0.2, max_features=2",
        "magenta",
        {"learning_rate": 0.2, "max_features": 2},
    ),
]:
    setting = dict(original_params)
    setting.update(setting)
    clf = ensemble.GradientBoostingClassifier(**setting)
    clf.fit(X_train, y_train)
```
