# Random Code Synthesis
Query `Visualize projections of CCA vs PLS.`
## Script Variables
- np:<br>
>The variable np is a Python library that provides a large collection of mathematical functions and data structures. It
- n_features:<br>
>n_features is the number of features in the dataset. It is used to calculate the mean and covariance
- i:<br>
>It is a 2D array of size (n_features, n_features) where n_features is
- X:<br>
>X is a 2x1500 matrix. It is the concatenation of two columns, x
- err_cov_emp_pure:<br>
>It is a matrix that contains the empirical covariance matrix of the pure outliers.
- j:<br>
>It is a random number generator that is used to generate random numbers for the outlier detection algorithm. The
- pure_X:<br>
>The variable pure_X is a matrix of size (n_samples, n_features) that contains the in
- pure_location:<br>
>It is a variable that contains the mean of the pure_X variable. Pure_X is a variable that
- inliers_mask:<br>
>It is a boolean array of shape (n_samples, n_features) that contains True for the in
- err_loc_emp_pure:<br>
>It is the error of the location estimation of the pure data set. It is used to compare the
- pure_emp_cov:<br>
>It is a variable that stores the EmpiricalCovariance object. This object is used to calculate
- EmpiricalCovariance:<br>
>EmpiricalCovariance is a class that calculates the empirical covariance matrix of a given dataset. It
- this_centroid:<br>
>This variable is a list of the centroids of the clusters generated by the BIRCH algorithm. It
- centroids:<br>
>Centroids are the points in the dataset that are considered to be the most representative of the data.
- zip:<br>
>It is a function that creates a list of tuples from a list of lists.
- info:<br>
>Variable info is a variable that is used to describe the role and significance of a variable within a
- birch_model:<br>
>Birch is a clustering algorithm that is used to cluster data points based on their similarity. It is
- colors_:<br>
>Colors_ is a list of colors used to color the points in the scatter plot. It is a
- k:<br>
>k is the number of clusters in the dataset. It is used to determine the number of centroids that
- mask:<br>
>The variable mask is a list of boolean values that represent the membership of each data point in a particular
- labels:<br>
>The variable labels are the labels assigned to each data point by the Birch model. The Birch model is
- print:<br>
>print() is a python function that prints the output to the console. It is used to display the
- fig:<br>
>fig is a matplotlib figure object which is used to create a plot. The figure object is used to
- ax:<br>
>The variable ax is a matplotlib.pyplot object that is used to plot the misclassification error on the test
- col:<br>
>col is a color that is used to represent the different clusters. It is used to color the points
- ind:<br>
>ind is a variable that is used to iterate through the different subplots that are created in the script
- range:<br>
>The variable range is from 1 to n_estimators + 1, where n_estimators is the number
- n_clusters:<br>
>n_clusters is a variable that is used to specify the number of clusters to be formed in the clustering
- hgbt:<br>
>hgbt is a HistGradientBoostingRegressor object which is used to perform gradient boosting regression.
- y_train:<br>
>The variable y_train is a numpy array containing the labels of the training data. It is used to
- _:<br>
>The variable _ is a placeholder for the number of iterations of the gradient boosting algorithm. It is used
- HistGradientBoostingRegressor:<br>
>HistGradientBoostingRegressor is a machine learning algorithm that uses historical data to predict future values. It
- plt:<br>
>plt is a python library that is used for plotting data. It is used to create graphs and charts
- X_train:<br>
>X_train is a numpy array of shape (n_samples, n_features) containing the training data.
- common_params:<br>
>It is a dictionary that contains the parameters that are common to all the trees in the ensemble. These
- max_iter:<br>
>The variable max_iter is a list of integers that represents the maximum number of iterations that the algorithm will
- cachedir:<br>
>The variable cachedir is a temporary directory that is used to store the memory cache of the BayesianR
- shutil:<br>
>shutil is a module in Python that provides a number of functions for working with files and directories.
- cat_selector:<br>
>It is a function that takes in a list of column names and returns a column transformer that can be
- make_column_transformer:<br>
>The make_column_transformer function is used to create a column transformer object that can be used to transform
- cat_tree_processor:<br>
>It is a pipeline that is used to preprocess the categorical data. The pipeline consists of two steps
- make_pipeline:<br>
>The make_pipeline function is used to create a pipeline of estimators. The pipeline is a sequence of
- num_selector:<br>
>num_selector is a function that takes a pandas dataframe as input and returns a column selector object. The
- num_tree_processor:<br>
>The num_tree_processor is a SimpleImputer object that is used to impute missing values in numerical
- SimpleImputer:<br>
>SimpleImputer is a class in sklearn.preprocessing that is used to impute missing values in numerical data
- tree_preprocessor:<br>
>Tree Preprocessor is a pipeline that takes a tree as input and returns a new tree with the same
- OrdinalEncoder:<br>
>OrdinalEncoder is a class used for encoding categorical data into numerical values. It is used for converting categorical
- prob_pos_clf:<br>
>Probabilities of the positive class as predicted by the classifier.
- clf:<br>
>clf is a gradient boosting classifier that is used to predict the class of a given sample. It is
- X_test:<br>
>X_test is a matrix containing the test data. It is used to test the model trained on the
- GaussianNB:<br>
>GaussianNB is a classifier that uses Bayes theorem to classify data points. It assumes that the
- lasso_pipeline:<br>
>The variable lasso_pipeline is a pipeline object that contains two components
- linear_preprocessor:<br>
>It is a linear preprocessor that is used to preprocess the data before fitting the model. It is
- LassoCV:<br>
>LassoCV is a linear regression model that uses the Lasso penalty to select a subset of features
- svm:<br>
>svm stands for Support Vector Machine. It is a supervised machine learning algorithm that is used for classification and
- y_valid:<br>
>It is a variable that contains the labels of the data points in the validation set. It is used
- RandomForestClassifier:<br>
>RandomForestClassifier is a machine learning algorithm that uses a decision tree ensemble to classify data. It is
- cal_clf:<br>
>CalibratedClassifierCV is a class that calibrates a classifier by fitting a separate calibration model to
- X_valid:<br>
>X_valid is a numpy array of size (n_samples, n_features) containing the features of the
- FrozenEstimator:<br>
>It is a class that implements the FrozenEstimator interface. This interface allows us to create a new
- y_test:<br>
>The variable y_test is a test set that is used to evaluate the performance of the model. It
- weak_learners_misclassification_error:<br>
>It is a function that calculates the misclassification error of a weak learner on the test set. This
- dummy_classifiers_misclassification_error:<br>
>dummy_classifiers_misclassification_error is a variable that represents the misclassification error of the dummy classifier
- y_pred:<br>
>The variable y_pred is the predicted value of the test set. It is used to calculate the mis
- n_estimators:<br>
>The variable n_estimators is the number of weak learners used in the AdaBoostClassifier algorithm. It is
- adaboost_clf:<br>
>Adaboost_clf is a classifier that is used to classify the data. It is a machine
- boosting_errors:<br>
>The boosting_errors variable is a pandas DataFrame that contains the number of trees in the AdaBoost algorithm and
- DecisionTreeClassifier:<br>
>DecisionTreeClassifier is a supervised learning algorithm that builds a decision tree from the training data. It is
- pd:<br>
>pd is a library that provides data structures and operations for manipulating tabular data in Python. It is
- misclassification_error:<br>
>Misclassification error is the percentage of misclassified examples in a dataset. It is calculated as the number
- DummyClassifier:<br>
>DummyClassifier is a class in scikit-learn that creates a dummy classifier. It is used to
- train_samples:<br>
>It is a variable that stores the number of samples to be used for training the model. This number
- label:<br>
>The variable label is a unique identifier for each observation in the dataset. It is used to identify the
- params:<br>
>The variable params is a dictionary that contains the parameters for the gradient boosting classifier. It includes the number
- color:<br>
>The variable color is used to represent the different colors of the different trees in the forest. The color
- index:<br>
>The variable index is a variable that is used to store the index of the data points in the dataset
- AgglomerativeClustering:<br>
>AgglomerativeClustering is a clustering algorithm that uses a bottom-up approach to form clusters. It
- linkage:<br>
>Variable linkage is a method used to identify the relationship between two or more variables. It is used to
- kneighbors_graph:<br>
>kneighbors_graph is a function that creates a graph of the k nearest neighbors of each point in the
- t0:<br>
>t0 is a variable that represents the time taken for the execution of the script.
- elapsed_time:<br>
>Elapsed time is the time taken by the program to execute a block of code. It is measured in
- model:<br>
>The variable model is a model that is used to predict the output of a function based on its input
- connectivity:<br>
>Connectivity is a variable that is used to determine the distance between two points in a graph. It
- dict:<br>
>dict is a python dictionary which is a collection of key-value pairs. The key is a string or
- time:<br>
>The variable time is used to measure the execution time of the script. It is used to calculate the
- enumerate:<br>
>Enumerate is a function that returns a sequence of tuples, where each tuple contains the index of the
## Synthesis Blocks
### notebooks/dataset2/ensemble_methods/plot_stack_predictors.ipynb
CONTEXT: Then, we will need to design preprocessing pipelines which depends on the ending regressor. If the ending regressor is a linear model, one
needs to one-hot encode the categories. If the ending regressor is a tree-based model an ordinal encoder will be sufficient. Besides, numerical values
need to be standardized for a linear model while the raw numerical data can be treated as is by a tree-based model. However, both models need an
imputer to handle missing values.  We will first design the pipeline required for the tree-based models.   COMMENT:
```python
from sklearn.compose import make_column_transformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import OrdinalEncoder
cat_tree_processor = OrdinalEncoder(
    handle_unknown="use_encoded_value",
    unknown_value=-1,
    encoded_missing_value=-2,
)
num_tree_processor = SimpleImputer(strategy="mean", add_indicator=True)
tree_preprocessor = make_column_transformer(
    (num_tree_processor, num_selector), (cat_tree_processor, cat_selector)
)
tree_preprocessor
```

### notebooks/dataset2/ensemble_methods/plot_stack_predictors.ipynb
CONTEXT:  Stack of predictors on a single data set  It is sometimes tedious to find the model which will best perform on a given dataset. Stacking
provide an alternative by combining the outputs of several learners, without the need to choose a model specifically. The performance of stacking is
usually close to the best model and sometimes it can outperform the prediction performance of each individual model.  Here, we combine 3 learners
(linear and non-linear) and use a ridge regressor to combine their outputs together.  <div class="alert alert-info"><h4>Note</h4><p>Although we will
make new pipelines with the processors which we wrote in    the previous section for the 3 learners, the final estimator
:class:`~sklearn.linear_model.RidgeCV()` does not need preprocessing of    the data as it will be fed with the already preprocessed output from the 3
learners.</p></div>   COMMENT:
```python
from sklearn.linear_model import LassoCV
lasso_pipeline = make_pipeline(linear_preprocessor, LassoCV())
lasso_pipeline
```

### notebooks/dataset2/calibration/plot_calibration_multiclass.ipynb
CONTEXT: To train the calibrated classifier, we start with the same :class:`~sklearn.ensemble.RandomForestClassifier` but train it using only the
train data subset (600 samples) then calibrate, with `method='sigmoid'`, using the valid data subset (400 samples) in a 2-stage process.   COMMENT:
```python
from sklearn.calibration import cal_clf
from sklearn.frozen import FrozenEstimator
clf = RandomForestClassifier(n_estimators=25)
clf.fit(X_train, y_train)
cal_clf = cal_clf(FrozenEstimator(clf), method="sigmoid")
cal_clf.fit(X_valid, y_valid)
```

### notebooks/dataset2/ensemble_methods/plot_adaboost_multiclass.ipynb
CONTEXT: After training the :class:`~sklearn.tree.DecisionTreeClassifier` model, the achieved error surpasses the expected value that would have been
obtained by guessing the most frequent class label, as the :class:`~sklearn.dummy.DummyClassifier` does.  Now, we calculate the
`misclassification_error`, i.e. `1 - accuracy`, of the additive model (:class:`~sklearn.tree.DecisionTreeClassifier`) at each boosting iteration on
the test set to assess its performance.  We use :meth:`~sklearn.ensemble.AdaBoostClassifier.staged_predict` that makes as many iterations as the
number of fitted estimator (i.e. corresponding to `n_estimators`). At iteration `n`, the predictions of AdaBoost only use the `n` first weak learners.
We compare these predictions with the true predictions `y_test` and we, therefore, conclude on the benefit (or not) of adding a new weak learner into
the chain.  We plot the misclassification error for the different stages:   COMMENT:
```python
import matplotlib.pyplot as plt
import pandas as pd
boosting_errors = pd.DataFrame(
    {
        "Number of trees": range(1, n_estimators + 1),
        "AdaBoost": [
            misclassification_error(y_test, y_pred)
            for y_pred in adaboost_clf.staged_predict(X_test)
        ],
    }
).set_index("Number of trees")
ax = boosting_errors.plot()
ax.set_ylabel("Misclassification error on test set")
ax.set_title("Convergence of AdaBoost algorithm")
plt.plot(
    [boosting_errors.index.min(), boosting_errors.index.max()],
    [weak_learners_misclassification_error, weak_learners_misclassification_error],
    color="tab:orange",
    linestyle="dashed",
)
plt.plot(
    [boosting_errors.index.min(), boosting_errors.index.max()],
    [
        dummy_classifiers_misclassification_error,
        dummy_classifiers_misclassification_error,
    ],
    color="c",
    linestyle="dotted",
)
plt.legend(["AdaBoost", "DecisionTreeClassifier", "DummyClassifier"], loc=1)
plt.show()
```

### notebooks/dataset2/calibration/plot_compare_calibration.ipynb
CONTEXT:   Comparison of Calibration of Classifiers  Well calibrated classifiers are probabilistic classifiers for which the output of
:term:`predict_proba` can be directly interpreted as a confidence level. For instance, a well calibrated (binary) classifier should classify the
samples such that for the samples to which it gave a :term:`predict_proba` value close to 0.8, approximately 80% actually belong to the positive
class.  In this example we will compare the calibration of four different models: `Logistic_regression`, `gaussian_naive_bayes`, `Random Forest
Classifier <forest>` and `Linear SVM <svm_classification>`. Authors: The scikit-learn developers SPDX-License-Identifier: BSD-3-Clause   COMMENT:
Samples used for training the models
```python
train_samples = 100
```

### notebooks/dataset2/classification/plot_digits_classification.ipynb
CONTEXT:  Classification  To apply a classifier on this data, we need to flatten the images, turning each 2-D array of grayscale values from shape
``(8, 8)`` into shape ``(64,)``. Subsequently, the entire dataset will be of shape ``(n_samples, n_features)``, where ``n_samples`` is the number of
images and ``n_features`` is the total number of pixels in each image.  We can then split the data into train and test subsets and fit a support
vector classifier on the train samples. The fitted classifier can subsequently be used to predict the value of the digit for the samples in the test
subset.   COMMENT: Create a classifier: a support vector classifier
```python
clf = svm.SVC(gamma=0.001)
```

### notebooks/dataset2/ensemble_methods/plot_gradient_boosting_regularization.ipynb
CONTEXT:   Gradient Boosting regularization  Illustration of the effect of different regularization strategies for Gradient Boosting. The example is
taken from Hastie et al 2009 [1]_.  The loss function used is binomial deviance. Regularization via shrinkage (``learning_rate < 1.0``) improves
performance considerably. In combination with shrinkage, stochastic gradient boosting (``subsample < 1.0``) can produce more accurate models by
reducing the variance via bagging. Subsampling without shrinkage usually does poorly. Another strategy to reduce the variance is by subsampling the
features analogous to the random splits in Random Forests (via the ``max_features`` parameter).  .. [1] T. Hastie, R. Tibshirani and J. Friedman,
"Elements of Statistical     Learning Ed. 2", Springer, 2009.  COMMENT: compute test set deviance
```python
    test_deviance = np.zeros((params["n_estimators"],), dtype=np.float64)
    for i, y_proba in enumerate(clf.staged_predict_proba(X_test)):
        test_deviance[i] = 2 * log_loss(y_test, y_proba[:, 1])
    plt.plot(
        (np.arange(test_deviance.shape[0]) + 1)[::5],
        test_deviance[::5],
        "-",
        color=color,
        label=label,
    )
plt.legend(loc="upper right")
plt.xlabel("Boosting Iterations")
plt.ylabel("Test Set Deviance")
plt.show()
```

### notebooks/dataset2/clustering/plot_feature_agglomeration_vs_univariate_selection.ipynb
CONTEXT: Attempt to remove the temporary cachedir, but don't worry if it fails   COMMENT:
```python
shutil.rmtree(cachedir, ignore_errors=True)
```

### notebooks/dataset2/ensemble_methods/plot_hgbt_regression.ipynb
CONTEXT: With just a few iterations, HGBT models can achieve convergence (see
`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`), meaning that adding more trees does not improve the model anymore. In
the figure above, 5 iterations are not enough to get good predictions. With 50 iterations, we are already able to do a good job.  Setting `max_iter`
too high might degrade the prediction quality and cost a lot of avoidable computing resources. Therefore, the HGBT implementation in scikit-learn
provides an automatic **early stopping** strategy. With it, the model uses a fraction of the training data as internal validation set
(`validation_fraction`) and stops training if the validation score does not improve (or degrades) after `n_iter_no_change` iterations up to a certain
tolerance (`tol`).  Notice that there is a trade-off between `learning_rate` and `max_iter`: Generally, smaller learning rates are preferable but
require more iterations to converge to the minimum loss, while larger learning rates converge faster (less iterations/trees needed) but at the cost of
a larger minimum loss.  Because of this high correlation between the learning rate the number of iterations, a good practice is to tune the learning
rate along with all (important) other hyperparameters, fit the HBGT on the training set with a large enough value for `max_iter` and determine the
best `max_iter` via early stopping and some explicit `validation_fraction`.   COMMENT:
```python
common_params = {
    "max_iter": 1_000,
    "learning_rate": 0.3,
    "validation_fraction": 0.2,
    "random_state": 42,
    "categorical_features": None,
    "scoring": "neg_root_mean_squared_error",
}
hgbt = HistGradientBoostingRegressor(early_stopping=True, **common_params)
hgbt.fit(X_train, y_train)
_, ax = plt.subplots()
plt.plot(-hgbt.validation_score_)
_ = ax.set(
    xlabel="number of iterations",
    ylabel="root mean squared error",
    title=f"Loss of hgbt with early stopping (n_iter={hgbt.n_iter_})",
)
```

### notebooks/dataset2/calibration/plot_calibration.ipynb
CONTEXT:  Gaussian Naive-Bayes   COMMENT: GaussianNB itself does not support sample-weights
```python
clf = GaussianNB()
clf.fit(X_train, y_train)

prob_pos_clf = clf.predict_proba(X_test)[:, 1]
```

### notebooks/dataset2/clustering/plot_agglomerative_clustering.ipynb
CONTEXT:   Agglomerative clustering with and without structure  This example shows the effect of imposing a connectivity graph to capture local
structure in the data. The graph is simply the graph of 20 nearest neighbors.  There are two advantages of imposing a connectivity. First, clustering
with sparse connectivity matrices is faster in general.  Second, when using a connectivity matrix, single, average and complete linkage are unstable
and tend to create a few clusters that grow very quickly. Indeed, average and complete linkage fight this percolation behavior by considering all the
distances between two clusters when merging them ( while single linkage exaggerates the behaviour by considering only the shortest distance between
clusters). The connectivity graph breaks this mechanism for average and complete linkage, making them resemble the more brittle single linkage. This
effect is more pronounced for very sparse graphs (try decreasing the number of neighbors in kneighbors_graph) and with complete linkage. In
particular, having a very small number of neighbors in the graph, imposes a geometry that is close to that of single linkage, which is well known to
have this percolation instability.  COMMENT: Create a graph capturing local connectivity. Larger number of neighbors will give more homogeneous
clusters to the cost of computation time. A very large number of neighbors gives more evenly distributed cluster sizes, but may not impose the local
manifold structure of the data
```python
kneighbors_graph = kneighbors_graph(X, 30, include_self=False)
for connectivity in (None, kneighbors_graph):
    for n_clusters in (30, 3):
        plt.figure(figsize=(10, 4))
        for index, linkage in enumerate(("average", "complete", "ward", "single")):
            plt.subplot(1, 4, index + 1)
            model = AgglomerativeClustering(
                linkage=linkage, connectivity=connectivity, n_clusters=n_clusters
            )
            t0 = time.time()
            model.fit(X)
            elapsed_time = time.time() - t0
            plt.scatter(X[:, 0], X[:, 1], c=model.labels_, cmap=plt.cm.nipy_spectral)
            plt.title(
                "linkage=%s\n(time %.2fs)" % (linkage, elapsed_time),
                fontdict=dict(verticalalignment="top"),
            )
            plt.axis("equal")
            plt.axis("off")
            plt.subplots_adjust(bottom=0, top=0.83, wspace=0, left=0, right=1)
            plt.suptitle(
                "n_cluster=%i, connectivity=%r"
                % (n_clusters, connectivity is not None),
                size=17,
            )
plt.show()
```

### notebooks/dataset2/covariance_estimation/plot_robust_vs_empirical_covariance.ipynb
CONTEXT:   Robust vs Empirical covariance estimate  The usual covariance maximum likelihood estimate is very sensitive to the presence of outliers in
the data set. In such a case, it would be better to use a robust estimator of covariance to guarantee that the estimation is resistant to "erroneous"
observations in the data set. [1]_, [2]_   Minimum Covariance Determinant Estimator The Minimum Covariance Determinant estimator is a robust, high-
breakdown point (i.e. it can be used to estimate the covariance matrix of highly contaminated datasets, up to $\frac{n_\text{samples} -
n_\text{features}-1}{2}$ outliers) estimator of covariance. The idea is to find $\frac{n_\text{samples} + n_\text{features}+1}{2}$ observations whose
empirical covariance has the smallest determinant, yielding a "pure" subset of observations from which to compute standards estimates of location and
covariance. After a correction step aiming at compensating the fact that the estimates were learned from only a portion of the initial data, we end up
with robust estimates of the data set location and covariance.  The Minimum Covariance Determinant estimator (MCD) has been introduced by P.J.Rousseuw
in [3]_.   Evaluation In this example, we compare the estimation errors that are made when using various types of location and covariance estimates on
contaminated Gaussian distributed data sets:  - The mean and the empirical covariance of the full dataset, which break   down as soon as there are
outliers in the data set - The robust MCD, that has a low error provided   $n_\text{samples} > 5n_\text{features}$ - The mean and the empirical
covariance of the observations that are known   to be good ones. This can be considered as a "perfect" MCD estimation,   so one can trust our
implementation by comparing to this case.    References .. [1] Johanna Hardin, David M Rocke. The distribution of robust distances.     Journal of
Computational and Graphical Statistics. December 1, 2005,     14(4): 928-946. .. [2] Zoubir A., Koivunen V., Chakhchoukh Y. and Muma M. (2012). Robust
estimation in signal processing: A tutorial-style treatment of     fundamental concepts. IEEE Signal Processing Magazine 29(4), 61-80. .. [3] P. J.
Rousseeuw. Least median of squares regression. Journal of American     Statistical Ass., 79:871, 1984.  COMMENT: compare with an empirical covariance
learned from a pure data set (i.e. "perfect" mcd)
```python
pure_X = X[inliers_mask]
pure_location = pure_X.mean(0)
pure_emp_cov = EmpiricalCovariance().fit(pure_X)
err_loc_emp_pure[i, j] = np.sum(pure_location**2)
err_cov_emp_pure[i, j] = pure_emp_cov.error_norm(np.eye(n_features))
```

### notebooks/dataset2/clustering/plot_birch_vs_minibatchkmeans.ipynb
CONTEXT:   Compare BIRCH and MiniBatchKMeans  This example compares the timing of BIRCH (with and without the global clustering step) and
MiniBatchKMeans on a synthetic dataset having 25,000 samples and 2 features generated using make_blobs.  Both ``MiniBatchKMeans`` and ``BIRCH`` are
very scalable algorithms and could run efficiently on hundreds of thousands or even millions of datapoints. We chose to limit the dataset size of this
example in the interest of keeping our Continuous Integration resource usage reasonable but the interested reader might enjoy editing this script to
rerun it with a larger value for `n_samples`.  If ``n_clusters`` is set to None, the data is reduced from 25,000 samples to a set of 158 clusters.
This can be viewed as a preprocessing step before the final (global) clustering step that further reduces these 158 clusters to 100 clusters.
COMMENT: Plot result
```python
    labels = birch_model.labels_
    centroids = birch_model.subcluster_centers_
    n_clusters = np.unique(labels).size
    print("n_clusters : %d" % n_clusters)
    ax = fig.add_subplot(1, 3, ind + 1)
    for this_centroid, k, col in zip(centroids, range(n_clusters), colors_):
        mask = labels == k
        ax.scatter(X[mask, 0], X[mask, 1], c="w", edgecolor=col, marker=".", alpha=0.5)
        if birch_model.n_clusters is None:
            ax.scatter(this_centroid[0], this_centroid[1], marker="+", c="k", s=25)
    ax.set_ylim([-25, 25])
    ax.set_xlim([-25, 25])
    ax.set_autoscaley_on(False)
    ax.set_title("BIRCH %s" % info)
```

## Code Concatenation
```python
from sklearn.compose import make_column_transformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import OrdinalEncoder
cat_tree_processor = OrdinalEncoder(
    handle_unknown="use_encoded_value",
    unknown_value=-1,
    encoded_missing_value=-2,
)
num_tree_processor = SimpleImputer(strategy="mean", add_indicator=True)
tree_preprocessor = make_column_transformer(
    (num_tree_processor, num_selector), (cat_tree_processor, cat_selector)
)
tree_preprocessor
from sklearn.linear_model import LassoCV
lasso_pipeline = make_pipeline(linear_preprocessor, LassoCV())
lasso_pipeline
from sklearn.calibration import cal_clf
from sklearn.frozen import FrozenEstimator
clf = RandomForestClassifier(n_estimators=25)
clf.fit(X_train, y_train)
cal_clf = cal_clf(FrozenEstimator(clf), method="sigmoid")
cal_clf.fit(X_valid, y_valid)
import matplotlib.pyplot as plt
import pandas as pd
boosting_errors = pd.DataFrame(
    {
        "Number of trees": range(1, n_estimators + 1),
        "AdaBoost": [
            misclassification_error(y_test, y_pred)
            for y_pred in adaboost_clf.staged_predict(X_test)
        ],
    }
).set_index("Number of trees")
ax = boosting_errors.plot()
ax.set_ylabel("Misclassification error on test set")
ax.set_title("Convergence of AdaBoost algorithm")
plt.plot(
    [boosting_errors.index.min(), boosting_errors.index.max()],
    [weak_learners_misclassification_error, weak_learners_misclassification_error],
    color="tab:orange",
    linestyle="dashed",
)
plt.plot(
    [boosting_errors.index.min(), boosting_errors.index.max()],
    [
        dummy_classifiers_misclassification_error,
        dummy_classifiers_misclassification_error,
    ],
    color="c",
    linestyle="dotted",
)
plt.legend(["AdaBoost", "DecisionTreeClassifier", "DummyClassifier"], loc=1)
plt.show()
train_samples = 100
clf = svm.SVC(gamma=0.001)
    test_deviance = np.zeros((params["n_estimators"],), dtype=np.float64)
    for i, y_proba in enumerate(clf.staged_predict_proba(X_test)):
        test_deviance[i] = 2 * log_loss(y_test, y_proba[:, 1])
    plt.plot(
        (np.arange(test_deviance.shape[0]) + 1)[::5],
        test_deviance[::5],
        "-",
        color=color,
        label=label,
    )
plt.legend(loc="upper right")
plt.xlabel("Boosting Iterations")
plt.ylabel("Test Set Deviance")
plt.show()
shutil.rmtree(cachedir, ignore_errors=True)
common_params = {
    "max_iter": 1_000,
    "learning_rate": 0.3,
    "validation_fraction": 0.2,
    "random_state": 42,
    "categorical_features": None,
    "scoring": "neg_root_mean_squared_error",
}
hgbt = HistGradientBoostingRegressor(early_stopping=True, **common_params)
hgbt.fit(X_train, y_train)
_, ax = plt.subplots()
plt.plot(-hgbt.validation_score_)
_ = ax.set(
    xlabel="number of iterations",
    ylabel="root mean squared error",
    title=f"Loss of hgbt with early stopping (n_iter={hgbt.n_iter_})",
)
clf = GaussianNB()
clf.fit(X_train, y_train)

prob_pos_clf = clf.predict_proba(X_test)[:, 1]
kneighbors_graph = kneighbors_graph(X, 30, include_self=False)
for connectivity in (None, kneighbors_graph):
    for n_clusters in (30, 3):
        plt.figure(figsize=(10, 4))
        for index, linkage in enumerate(("average", "complete", "ward", "single")):
            plt.subplot(1, 4, index + 1)
            model = AgglomerativeClustering(
                linkage=linkage, connectivity=connectivity, n_clusters=n_clusters
            )
            t0 = time.time()
            model.fit(X)
            elapsed_time = time.time() - t0
            plt.scatter(X[:, 0], X[:, 1], c=model.labels_, cmap=plt.cm.nipy_spectral)
            plt.title(
                "linkage=%s\n(time %.2fs)" % (linkage, elapsed_time),
                fontdict=dict(verticalalignment="top"),
            )
            plt.axis("equal")
            plt.axis("off")
            plt.subplots_adjust(bottom=0, top=0.83, wspace=0, left=0, right=1)
            plt.suptitle(
                "n_cluster=%i, connectivity=%r"
                % (n_clusters, connectivity is not None),
                size=17,
            )
plt.show()
pure_X = X[inliers_mask]
pure_location = pure_X.mean(0)
pure_emp_cov = EmpiricalCovariance().fit(pure_X)
err_loc_emp_pure[i, j] = np.sum(pure_location**2)
err_cov_emp_pure[i, j] = pure_emp_cov.error_norm(np.eye(n_features))
    labels = birch_model.labels_
    centroids = birch_model.subcluster_centers_
    n_clusters = np.unique(labels).size
    print("n_clusters : %d" % n_clusters)
    ax = fig.add_subplot(1, 3, ind + 1)
    for this_centroid, k, col in zip(centroids, range(n_clusters), colors_):
        mask = labels == k
        ax.scatter(X[mask, 0], X[mask, 1], c="w", edgecolor=col, marker=".", alpha=0.5)
        if birch_model.n_clusters is None:
            ax.scatter(this_centroid[0], this_centroid[1], marker="+", c="k", s=25)
    ax.set_ylim([-25, 25])
    ax.set_xlim([-25, 25])
    ax.set_autoscaley_on(False)
    ax.set_title("BIRCH %s" % info)
```
